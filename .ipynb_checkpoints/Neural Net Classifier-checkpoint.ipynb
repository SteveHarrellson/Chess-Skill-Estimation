{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the FEN data, define the test_train dataset, and pass to a neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\requests\\__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_10 = pd.read_csv('Data\\\\Standard 1000-1199\\\\hot1.csv')\n",
    "Dataset_12 = pd.read_csv('Data\\\\Standard 1200-1399\\\\hot1.csv')\n",
    "Dataset_14 = pd.read_csv('Data\\\\Standard 1400-1599\\\\hot1.csv')\n",
    "Dataset_16 = pd.read_csv('Data\\\\Standard 1600-1799\\\\hot1.csv')\n",
    "Dataset_18 = pd.read_csv('Data\\\\Standard 1800-1999\\\\hot1.csv')\n",
    "Dataset_20 = pd.read_csv('Data\\\\Standard 2000-2199\\\\hot1.csv')\n",
    "Dataset_22 = pd.read_csv('Data\\\\Standard 2200-2399\\\\hot1.csv')\n",
    "Dataset_24 = pd.read_csv('Data\\\\Standard 2400-2599\\\\hot1.csv')\n",
    "\n",
    "\n",
    "Dataset_list = [Dataset_10,\n",
    "               Dataset_12,\n",
    "               Dataset_14,\n",
    "               Dataset_16,\n",
    "               Dataset_18,\n",
    "               Dataset_20,\n",
    "               Dataset_22,\n",
    "               Dataset_24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_Label_Extraction(df_in,num_of_examples = 3746):\n",
    "    df_out = df_in[0:num_of_examples]\n",
    "    df_out_feature = df_out.iloc[:,:832]\n",
    "    \n",
    "    white_elos = df_out[\"White ELO\"].astype(int)\n",
    "    black_elos = df_out[\"Black ELO\"].astype(int)\n",
    "    avg_elos = np.mean([np.array(white_elos),np.array(black_elos)],axis=0)\n",
    "    \n",
    "    df_out_label = pd.DataFrame(columns = [\"Average ELO\"])\n",
    "    df_out_label[\"Average ELO\"] = avg_elos\n",
    "    \n",
    "    #df_out_label = white_elos\n",
    "    return df_out_feature , df_out_label\n",
    "    \n",
    "def Prepare_Datasets(dataset_list):\n",
    "    out_df_feature = pd.DataFrame()\n",
    "    out_df_label = pd.DataFrame()\n",
    "    for dataframe in dataset_list:\n",
    "        feature , label = Feature_Label_Extraction(dataframe)\n",
    "        out_df_feature = out_df_feature.append(feature,ignore_index=True)\n",
    "        #out_df_label = out_df_label.append(label,ignore_index = True)\n",
    "        out_df_label = pd.concat([out_df_label,label])\n",
    "        print(np.size(label))\n",
    "    return out_df_feature , out_df_label\n",
    "\n",
    "\n",
    "def Hot_1_encoder(df_in):\n",
    "    New_array = np.zeros([df_in.shape[0],df_in.shape[1]*13])\n",
    "    for i in range(df_in.shape[0]):\n",
    "        for j,val in enumerate(df_in.iloc[i][:]):\n",
    "            New_array[i,13*j+int(val)] = 1\n",
    "            \n",
    "    return New_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx, trainy = Prepare_Datasets(Dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainy.min(),trainy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize y\n",
    "trainy = (trainy- trainy.min())/(trainy.max()-trainy.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(trainx, trainy, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with tf.device('/DML:0'): \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(832,)),\n",
    "    keras.layers.Dense(832, activation = \"relu\"),\n",
    "    keras.layers.Dense(832, activation = \"relu\"),\n",
    "    keras.layers.Dense(512, activation = \"relu\"),\n",
    "    keras.layers.Dense(256, activation = \"relu\"),\n",
    "    keras.layers.Dense(128, activation = \"relu\"),\n",
    "    keras.layers.Dense(64, activation = \"relu\"),\n",
    "    keras.layers.Dense(32, activation = \"relu\"),\n",
    "    keras.layers.Dense(32, activation = \"relu\"),\n",
    "    keras.layers.Dense(32, activation = \"relu\"),\n",
    "    keras.layers.Dense(32, activation = \"relu\"),\n",
    "    keras.layers.Dense(32, activation = \"relu\"),\n",
    "    keras.layers.Dense(32, activation = \"relu\"),\n",
    "    keras.layers.Dense(16, activation = \"relu\"),\n",
    "    keras.layers.Dense(8, activation = \"relu\"),\n",
    "    keras.layers.Dense(4, activation = \"relu\"),\n",
    "    keras.layers.Dense(1, activation = \"linear\")\n",
    "])\n",
    "\n",
    "#custom_optimizer=tf.keras.optimizers.SGD(learning_rate=0.02)\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer = keras.optimizers.Adam(1e-3), metrics=['mean_squared_error'],)\n",
    "\n",
    "history = model.fit(X_train,y_train, epochs = 5, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "\n",
    "mse = history.history['mean_squared_error']\n",
    "val_mse = history.history['val_mean_squared_error']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(mse) + 1)\n",
    "\n",
    "plt.plot(epochs, mse, 'bo', label='Training mse')\n",
    "plt.plot(epochs, val_mse, 'b', label='Validation mse')\n",
    "plt.title('Training and validation MSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(model.predict_on_batch(X_test),y_test,c='#88c999',s=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(model.predict_on_batch(X_test)[0],y_test[0])\n",
    "#y_test.iloc[0,1]\n",
    "#np.size(model.predict_on_batch(X_test)[:][0])\n",
    "#print(model.predict_on_batch(X_test)[3][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
